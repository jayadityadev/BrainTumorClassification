# ğŸ”¬ Complete Reproduction Guide

This guide will walk you through reproducing the entire Brain Tumor Classification project from scratch.

## ğŸ“‹ Prerequisites

### System Requirements
- **OS**: Linux (Ubuntu 20.04+), macOS, or Windows with WSL2
- **Python**: 3.9 or higher
- **RAM**: 16GB minimum (32GB recommended)
- **GPU**: NVIDIA GPU with 8GB+ VRAM (optional but highly recommended)
  - CUDA 11.8+ and cuDNN 8.6+
- **Storage**: ~20GB free space

### Software Requirements
- Python 3.9+
- pip (Python package manager)
- virtualenv or conda
- Git (for version control)

---

## ğŸš€ Step-by-Step Reproduction

### Step 1: Clone and Setup Environment

```bash
# Clone the repository
git clone <your-repo-url>
cd BrainTumorProject

# Create virtual environment
python3 -m venv .venv

# Activate virtual environment
# On Linux/macOS:
source .venv/bin/activate
# On Windows:
.venv\Scripts\activate

# Upgrade pip
pip install --upgrade pip

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Verify GPU Setup (Optional but Recommended)

```python
# Check TensorFlow GPU availability
python -c "import tensorflow as tf; print('GPUs:', tf.config.list_physical_devices('GPU'))"
```

Expected output:
```
GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

If no GPU is detected but you have one:
1. Install NVIDIA drivers
2. Install CUDA Toolkit 11.8+
3. Install cuDNN 8.6+
4. Reinstall TensorFlow: `pip install tensorflow[and-cuda]`

### Step 3: Create Project Directories

```bash
# Run the directory setup script
python setup_directories.py
```

This creates all necessary folders:
- `data/` - Raw and processed data
- `models/` - Model weights and checkpoints
- `outputs/` - Predictions, logs, reports
- `config/` - Configuration files
- etc.

### Step 4: Obtain the Dataset

You need brain tumor MRI datasets. Two options:

#### Option A: Original Dataset (.mat files)
1. Download the Brain Tumor dataset (MATLAB .mat format)
2. Place all `.mat` files in `dataset/` directory
3. Expected structure:
   ```
   dataset/
   â”œâ”€â”€ 1.mat
   â”œâ”€â”€ 2.mat
   â”œâ”€â”€ 3.mat
   â””â”€â”€ ... (3000+ files)
   ```

#### Option B: Kaggle Dataset (images)
1. Download from Kaggle: [Brain Tumor Classification MRI](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset)
2. Extract to `kaggle_temp/` directory
3. Expected structure:
   ```
   kaggle_temp/
   â”œâ”€â”€ Training/
   â”‚   â”œâ”€â”€ glioma/
   â”‚   â”œâ”€â”€ meningioma/
   â”‚   â”œâ”€â”€ notumor/
   â”‚   â””â”€â”€ pituitary/
   â””â”€â”€ Testing/
       â”œâ”€â”€ glioma/
       â”œâ”€â”€ meningioma/
       â”œâ”€â”€ notumor/
       â””â”€â”€ pituitary/
   ```

### Step 5: Data Preprocessing

#### 5.1: Convert MATLAB Files to Images (if using .mat files)

```bash
python src/preprocessing/convert_mat_to_png.py
```

This will:
- Convert `.mat` files to PNG images
- Extract tumor masks
- Save to `data/raw/`
- Create a metadata CSV file

Expected output:
```
Processing 3064 .mat files...
âœ“ Converted: 3064 images
âœ“ Output: data/raw/images/
âœ“ Metadata: data/raw/metadata.csv
```

#### 5.2: Enhance Images (Critical Step!)

```bash
python src/preprocessing/enhance.py
```

This applies:
- **Non-Local Means Denoising** - Removes noise while preserving edges
- **CLAHE** (Contrast Limited Adaptive Histogram Equalization) - Enhances contrast

Expected output:
```
Processing images with multiprocessing...
âœ“ Enhanced: 3064 images
âœ“ Output: data/processed/
```

**Note**: This step is crucial for achieving high accuracy (99%+). The enhancement removes domain shift issues.

### Step 6: Combine Datasets (Optional)

If you have both datasets, combine them:

```bash
python src/data/combine_datasets.py
```

This merges:
- Original dataset (data/processed/)
- Kaggle dataset (kaggle_temp/)
- Creates unified structure in `data/combined/`

### Step 7: Train the Model

#### Option A: Quick Fine-tuning (Recommended)

If you have the Kaggle dataset:

```bash
python src/models/fast_finetune_kaggle.py
```

**Training Parameters:**
- Base Model: DenseNet121 (pretrained on ImageNet)
- Image Size: 224Ã—224
- Batch Size: 32
- Learning Rate: 1e-4
- Augmentation: âœ“ Enabled
- Early Stopping: âœ“ Enabled

**Expected Training Time:**
- GPU (RTX 3080): ~15-20 minutes
- GPU (RTX 4090): ~8-12 minutes
- CPU: ~2-4 hours (not recommended)

**Expected Output:**
```
Epoch 1/50: loss: 0.4521 - accuracy: 0.8532 - val_loss: 0.2341 - val_accuracy: 0.9234
Epoch 2/50: loss: 0.2134 - accuracy: 0.9245 - val_loss: 0.1523 - val_accuracy: 0.9567
...
Epoch 18/50: loss: 0.0234 - accuracy: 0.9912 - val_loss: 0.0189 - val_accuracy: 0.9923
Early stopping triggered!

âœ“ Model saved: models/current/densenet121_finetuned.keras
âœ“ Training complete! Final accuracy: 99.23%
```

#### Option B: Full Training (Combined Dataset)

```bash
python src/models/train_combined_dataset.py
```

Uses both datasets for maximum performance.

### Step 8: Create Configuration Files

Create `config/model_config.json`:

```json
{
    "model_name": "densenet121",
    "input_shape": [224, 224, 3],
    "num_classes": 4,
    "learning_rate": 0.0001,
    "batch_size": 32,
    "epochs": 50,
    "class_names": ["glioma", "meningioma", "notumor", "pituitary"]
}
```

Create `config/augmentation_config.json`:

```json
{
    "rotation_range": 15,
    "width_shift_range": 0.1,
    "height_shift_range": 0.1,
    "zoom_range": 0.1,
    "horizontal_flip": true,
    "fill_mode": "nearest"
}
```

### Step 9: Validate the System

```bash
python scripts/validate_system.py
```

This runs 10 comprehensive tests:
1. âœ“ GPU detection
2. âœ“ Model loading
3. âœ“ Inference on sample image
4. âœ“ Preprocessing pipeline
5. âœ“ Grad-CAM visualization
6. âœ“ Web app configuration
7. âœ“ Output directories
8. âœ“ File structure
9. âœ“ Image enhancement
10. âœ“ Model performance

Expected output:
```
=== System Validation Results ===
âœ“ Test 1: GPU Detection - PASSED
âœ“ Test 2: Model Loading - PASSED
...
âœ“ Test 10: Model Performance - PASSED

Overall: 10/10 tests passed âœ…
System is ready for production!
```

### Step 10: Evaluate Model Performance (Optional)

```bash
python scripts/evaluate_kaggle.py
```

Generates:
- Confusion matrix
- Classification report
- Per-class accuracy
- Saved to `outputs/reports/`

Expected metrics:
```
Overall Accuracy: 99.23%

Per-Class Accuracy:
  - Glioma: 99.1%
  - Meningioma: 99.5%
  - No Tumor: 100.0%
  - Pituitary: 98.8%

Precision: 99.24%
Recall: 99.23%
F1-Score: 99.23%
```

### Step 11: Launch the Web Application

```bash
python app.py
```

Access the web interface at: **http://localhost:5000**

**Features:**
- Upload MRI images
- Real-time prediction
- Grad-CAM visualization (shows which brain regions influenced the decision)
- Confidence scores for all classes
- Save predictions

---

## ğŸ“Š Expected Results

### Model Performance
- **Accuracy**: 99.23%
- **Training Time**: 15-20 minutes (GPU)
- **Inference Time**: ~50ms per image
- **Model Size**: 90MB

### Dataset Statistics
- **Training Samples**: ~7,000 images
- **Validation Samples**: ~1,500 images
- **Test Samples**: ~1,000 images
- **Classes**: 4 (Glioma, Meningioma, No Tumor, Pituitary)

---

## ğŸ› Troubleshooting

### Issue: Low Accuracy (<90%)

**Cause**: Missing image enhancement step

**Solution**:
```bash
# Make sure you ran the enhancement
python src/preprocessing/enhance.py

# Re-train with enhanced images
python src/models/fast_finetune_kaggle.py
```

### Issue: GPU Not Detected

**Cause**: Missing CUDA/cuDNN or driver issues

**Solution**:
```bash
# Check NVIDIA driver
nvidia-smi

# Reinstall TensorFlow with GPU support
pip uninstall tensorflow
pip install tensorflow[and-cuda]

# Verify
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

### Issue: Out of Memory (OOM)

**Cause**: Batch size too large for your GPU

**Solution**:
Edit `src/models/fast_finetune_kaggle.py`:
```python
# Change this line
BATCH_SIZE = 32  # Change to 16 or 8
```

### Issue: Web App Not Loading

**Cause**: Model file not found

**Solution**:
```bash
# Check model exists
ls models/current/

# If missing, retrain or download pre-trained model
python src/models/fast_finetune_kaggle.py
```

### Issue: Import Errors

**Cause**: Missing dependencies

**Solution**:
```bash
# Reinstall all dependencies
pip install -r requirements.txt --force-reinstall
```

---

## ğŸ”„ Reproducing Specific Components

### Just the Preprocessing
```bash
python src/preprocessing/convert_mat_to_png.py
python src/preprocessing/enhance.py
```

### Just the Training
```bash
# Make sure data is preprocessed first
python src/models/fast_finetune_kaggle.py
```

### Just the Web App
```bash
# Make sure model exists in models/current/
python app.py
```

### Just the Evaluation
```bash
python scripts/evaluate_kaggle.py
```

---

## ğŸ“¦ Project Structure After Setup

```
BrainTumorProject/
â”œâ”€â”€ app.py                          # Web application
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup_directories.py            # Directory setup script
â”œâ”€â”€ README.md                       # Project overview
â”œâ”€â”€ REPRODUCTION_GUIDE.md           # This file
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â”œâ”€â”€ model_config.json
â”‚   â””â”€â”€ augmentation_config.json
â”‚
â”œâ”€â”€ data/                           # Data directories
â”‚   â”œâ”€â”€ raw/                        # Converted images
â”‚   â”œâ”€â”€ processed/                  # Enhanced images
â”‚   â””â”€â”€ splits/                     # Train/val/test splits
â”‚
â”œâ”€â”€ dataset/                        # Original .mat files
â”œâ”€â”€ kaggle_temp/                    # Kaggle dataset
â”‚
â”œâ”€â”€ models/                         # Model storage
â”‚   â”œâ”€â”€ current/                    # Production model
â”‚   â”‚   â””â”€â”€ densenet121_finetuned.keras
â”‚   â””â”€â”€ archive/                    # Old versions
â”‚
â”œâ”€â”€ outputs/                        # Generated outputs
â”‚   â”œâ”€â”€ predictions/                # Saved predictions
â”‚   â”œâ”€â”€ logs/                       # Training logs
â”‚   â”œâ”€â”€ reports/                    # Evaluation reports
â”‚   â””â”€â”€ training_history/           # Training metrics
â”‚
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ preprocessing/              # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ convert_mat_to_png.py
â”‚   â”‚   â””â”€â”€ enhance.py
â”‚   â”œâ”€â”€ models/                     # Training scripts
â”‚   â”‚   â”œâ”€â”€ train_combined_dataset.py
â”‚   â”‚   â””â”€â”€ fast_finetune_kaggle.py
â”‚   â”œâ”€â”€ inference/                  # Prediction & visualization
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ gradcam.py
â”‚   â””â”€â”€ data/                       # Data utilities
â”‚       â””â”€â”€ combine_datasets.py
â”‚
â”œâ”€â”€ scripts/                        # Utility scripts
â”‚   â”œâ”€â”€ validate_system.py          # System validation
â”‚   â””â”€â”€ evaluate_kaggle.py          # Model evaluation
â”‚
â”œâ”€â”€ templates/                      # Web app templates
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_inference.py
â”‚
â””â”€â”€ docs/                           # Documentation
    â”œâ”€â”€ ARCHITECTURE.md             # System architecture
    â””â”€â”€ SETUP.md                    # Installation guide
```

---

## âš¡ Quick Start (TL;DR)

```bash
# 1. Setup
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python setup_directories.py

# 2. Get data (place in dataset/ or kaggle_temp/)

# 3. Preprocess (if using .mat files)
python src/preprocessing/convert_mat_to_png.py
python src/preprocessing/enhance.py

# 4. Train
python src/models/fast_finetune_kaggle.py

# 5. Validate
python scripts/validate_system.py

# 6. Run
python app.py
```

---

## ğŸ“ Notes

1. **Image Enhancement is Critical**: The preprocessing step (Non-Local Means + CLAHE) is essential for achieving 99%+ accuracy. Without it, accuracy drops to ~85-90%.

2. **GPU Recommended**: Training on CPU will take 10-20x longer. A GPU with 8GB+ VRAM is highly recommended.

3. **Dataset Sources**: You can use either the original .mat dataset or the Kaggle image dataset. Both achieve similar performance.

4. **Model Architecture**: DenseNet121 was chosen after testing ResNet50 and EfficientNet. It provides the best balance of accuracy and speed.

5. **Reproducibility**: All random seeds are set for reproducibility. You should get identical results (Â±0.5%) on the same hardware.

---

## ğŸ¯ Success Criteria

You've successfully reproduced the project when:
- âœ“ All 10 validation tests pass
- âœ“ Model accuracy â‰¥ 99%
- âœ“ Web app runs without errors
- âœ“ Grad-CAM visualization works
- âœ“ Inference time < 100ms per image

---

## ğŸ“§ Support

If you encounter issues:
1. Check the Troubleshooting section above
2. Verify all prerequisites are installed
3. Ensure GPU drivers are up to date
4. Check that data is preprocessed correctly

---

**Last Updated**: October 26, 2025  
**Project Version**: 2.0  
**Python Version**: 3.9+  
**TensorFlow Version**: 2.13+
