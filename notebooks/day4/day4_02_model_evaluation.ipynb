{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a513cfd6",
   "metadata": {},
   "source": [
    "# Day 4.2: Model Evaluation\n",
    "\n",
    "**Goal:** Comprehensively evaluate the trained model on the test set\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load the best trained model\n",
    "2. Make predictions on test set\n",
    "3. Generate confusion matrix\n",
    "4. Compute classification metrics (precision, recall, F1-score)\n",
    "5. Analyze per-class performance\n",
    "6. Identify misclassifications\n",
    "7. Save evaluation report\n",
    "\n",
    "**Expected time:** 15-20 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70570900",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Scikit-learn for metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8941ad1",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15895e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_CSV = \"../../outputs/data_splits/test_split.csv\"\n",
    "MODEL_DIR = \"../../outputs/models\"\n",
    "VIZ_DIR = \"../../outputs/visualizations\"\n",
    "RESULTS_DIR = \"../../outputs/evaluation_results\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(VIZ_DIR, exist_ok=True)\n",
    "\n",
    "# Model parameters\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Class names\n",
    "CLASS_NAMES = ['glioma', 'meningioma', 'pituitary']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "print(\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab463849",
   "metadata": {},
   "source": [
    "## 3. Find and Load Best Model\n",
    "\n",
    "This will load the most recent trained model from Day 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9450d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most recent model file\n",
    "model_files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.keras')]\n",
    "\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\"No trained model found! Please run day4_01_full_training.ipynb first.\")\n",
    "\n",
    "# Sort by modification time (most recent first)\n",
    "model_files.sort(key=lambda x: os.path.getmtime(os.path.join(MODEL_DIR, x)), reverse=True)\n",
    "latest_model = model_files[0]\n",
    "model_path = os.path.join(MODEL_DIR, latest_model)\n",
    "\n",
    "print(f\"Loading model: {latest_model}\")\n",
    "print(f\"Path: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "model = keras.models.load_model(model_path)\n",
    "print(\"\\n✅ Model loaded successfully!\")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a0f0e",
   "metadata": {},
   "source": [
    "## 4. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c94b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test CSV\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "test_df['label'] = test_df['label'].astype(str)\n",
    "\n",
    "print(f\"Test set: {len(test_df)} images from {test_df['patient_id'].nunique()} patients\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# Create test generator (no augmentation)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filepath',\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    color_mode='grayscale',\n",
    "    shuffle=False  # Important: keep order for predictions\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Test generator created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc050624",
   "metadata": {},
   "source": [
    "## 5. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962da17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔮 Making predictions on test set...\\n\")\n",
    "\n",
    "# Get predictions (probabilities for each class)\n",
    "y_pred_probs = model.predict(test_generator, verbose=1)\n",
    "\n",
    "# Get predicted class indices\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Get true class indices\n",
    "y_true_classes = test_generator.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "print(f\"\\n✅ Predictions completed!\")\n",
    "print(f\"Shape of predictions: {y_pred_probs.shape}\")\n",
    "print(f\"Number of samples: {len(y_pred_classes)}\")\n",
    "print(f\"Class labels: {class_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8ec75",
   "metadata": {},
   "source": [
    "## 6. Calculate Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b07de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"📊 TEST SET ACCURACY: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate number of correct/incorrect predictions\n",
    "correct = np.sum(y_pred_classes == y_true_classes)\n",
    "incorrect = len(y_pred_classes) - correct\n",
    "\n",
    "print(f\"\\nCorrect predictions: {correct} / {len(y_pred_classes)}\")\n",
    "print(f\"Incorrect predictions: {incorrect} / {len(y_pred_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7a784",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eef042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "cm_path = os.path.join(VIZ_DIR, 'day4_02_confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Confusion matrix saved to: {cm_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed0dc60",
   "metadata": {},
   "source": [
    "## 8. Normalized Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized confusion matrix (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_normalized, \n",
    "    annot=True, \n",
    "    fmt='.2%', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_labels,\n",
    "    yticklabels=class_labels,\n",
    "    cbar_kws={'label': 'Percentage'}\n",
    ")\n",
    "plt.title('Normalized Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "cm_norm_path = os.path.join(VIZ_DIR, 'day4_02_confusion_matrix_normalized.png')\n",
    "plt.savefig(cm_norm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Normalized confusion matrix saved to: {cm_norm_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fc7ba",
   "metadata": {},
   "source": [
    "## 9. Classification Report (Precision, Recall, F1-Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c03e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    y_true_classes, \n",
    "    y_pred_classes, \n",
    "    target_names=class_labels,\n",
    "    digits=4\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 CLASSIFICATION REPORT\")\n",
    "print(\"=\"*70)\n",
    "print(report)\n",
    "\n",
    "# Save report to file\n",
    "report_path = os.path.join(RESULTS_DIR, 'classification_report.txt')\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"Classification Report - Brain Tumor Classification\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    f.write(report)\n",
    "    f.write(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(f\"\\nClassification report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a0419",
   "metadata": {},
   "source": [
    "## 10. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665e244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true_classes, \n",
    "    y_pred_classes, \n",
    "    labels=range(NUM_CLASSES)\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_labels,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Plot per-class metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "metrics_path = os.path.join(VIZ_DIR, 'day4_02_per_class_metrics.png')\n",
    "plt.savefig(metrics_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nPer-class metrics plot saved to: {metrics_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486e543",
   "metadata": {},
   "source": [
    "## 11. Identify Misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda59e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified samples\n",
    "misclassified_indices = np.where(y_pred_classes != y_true_classes)[0]\n",
    "\n",
    "print(f\"Total misclassifications: {len(misclassified_indices)} / {len(y_pred_classes)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_indices)/len(y_pred_classes)*100:.2f}%\")\n",
    "\n",
    "# Create DataFrame of misclassifications\n",
    "misclassified_data = []\n",
    "for idx in misclassified_indices:\n",
    "    true_label = class_labels[y_true_classes[idx]]\n",
    "    pred_label = class_labels[y_pred_classes[idx]]\n",
    "    confidence = y_pred_probs[idx][y_pred_classes[idx]]\n",
    "    filepath = test_df.iloc[idx]['filepath']\n",
    "    patient_id = test_df.iloc[idx]['patient_id']\n",
    "    \n",
    "    misclassified_data.append({\n",
    "        'index': idx,\n",
    "        'filepath': filepath,\n",
    "        'patient_id': patient_id,\n",
    "        'true_label': true_label,\n",
    "        'predicted_label': pred_label,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "misclassified_df = pd.DataFrame(misclassified_data)\n",
    "\n",
    "# Save misclassifications\n",
    "misclass_csv = os.path.join(RESULTS_DIR, 'misclassified_samples.csv')\n",
    "misclassified_df.to_csv(misclass_csv, index=False)\n",
    "print(f\"\\nMisclassified samples saved to: {misclass_csv}\")\n",
    "\n",
    "# Show first 10 misclassifications\n",
    "print(\"\\nFirst 10 misclassifications:\")\n",
    "print(misclassified_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476d483",
   "metadata": {},
   "source": [
    "## 12. Analyze Prediction Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ddea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence scores for predicted classes\n",
    "confidence_scores = np.max(y_pred_probs, axis=1)\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_mask = y_pred_classes == y_true_classes\n",
    "correct_confidences = confidence_scores[correct_mask]\n",
    "incorrect_confidences = confidence_scores[~correct_mask]\n",
    "\n",
    "# Plot confidence distributions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(correct_confidences, bins=30, alpha=0.7, label='Correct', color='green')\n",
    "ax1.hist(incorrect_confidences, bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "ax1.set_xlabel('Confidence Score', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(\n",
    "    [correct_confidences, incorrect_confidences],\n",
    "    labels=['Correct', 'Incorrect'],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor='lightblue', alpha=0.7)\n",
    ")\n",
    "ax2.set_ylabel('Confidence Score', fontsize=12)\n",
    "ax2.set_title('Confidence Score Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "conf_path = os.path.join(VIZ_DIR, 'day4_02_confidence_analysis.png')\n",
    "plt.savefig(conf_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Confidence analysis saved to: {conf_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"Correct predictions - Mean: {correct_confidences.mean():.4f}, Std: {correct_confidences.std():.4f}\")\n",
    "print(f\"Incorrect predictions - Mean: {incorrect_confidences.mean():.4f}, Std: {incorrect_confidences.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311fa59",
   "metadata": {},
   "source": [
    "## 13. Save Complete Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aebaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "evaluation_results = {\n",
    "    'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'model_path': model_path,\n",
    "    'test_samples': len(test_df),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'correct_predictions': int(correct),\n",
    "    'incorrect_predictions': int(incorrect),\n",
    "    'per_class_metrics': {\n",
    "        class_labels[i]: {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1_score': float(f1[i]),\n",
    "            'support': int(support[i])\n",
    "        }\n",
    "        for i in range(NUM_CLASSES)\n",
    "    },\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'confidence_stats': {\n",
    "        'correct': {\n",
    "            'mean': float(correct_confidences.mean()),\n",
    "            'std': float(correct_confidences.std()),\n",
    "            'min': float(correct_confidences.min()),\n",
    "            'max': float(correct_confidences.max())\n",
    "        },\n",
    "        'incorrect': {\n",
    "            'mean': float(incorrect_confidences.mean()),\n",
    "            'std': float(incorrect_confidences.std()),\n",
    "            'min': float(incorrect_confidences.min()),\n",
    "            'max': float(incorrect_confidences.max())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "results_json = os.path.join(RESULTS_DIR, 'evaluation_results.json')\n",
    "with open(results_json, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"Complete evaluation results saved to: {results_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7594bb8",
   "metadata": {},
   "source": [
    "## 14. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba88791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 DAY 4.2 COMPLETE - EVALUATION FINISHED!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n📊 Summary:\")\n",
    "print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"  Total Samples: {len(test_df)}\")\n",
    "print(f\"  Correct: {correct}\")\n",
    "print(f\"  Incorrect: {incorrect}\")\n",
    "\n",
    "print(\"\\n📁 Files Created:\")\n",
    "print(f\"  ✅ Confusion Matrix:           {cm_path}\")\n",
    "print(f\"  ✅ Normalized Confusion Matrix: {cm_norm_path}\")\n",
    "print(f\"  ✅ Per-Class Metrics Plot:     {metrics_path}\")\n",
    "print(f\"  ✅ Confidence Analysis:         {conf_path}\")\n",
    "print(f\"  ✅ Classification Report:       {report_path}\")\n",
    "print(f\"  ✅ Misclassified Samples:       {misclass_csv}\")\n",
    "print(f\"  ✅ Evaluation Results JSON:     {results_json}\")\n",
    "\n",
    "print(\"\\n🎯 Key Findings:\")\n",
    "for i, class_name in enumerate(class_labels):\n",
    "    print(f\"  {class_name.capitalize()}:\")\n",
    "    print(f\"    - Precision: {precision[i]:.4f}\")\n",
    "    print(f\"    - Recall:    {recall[i]:.4f}\")\n",
    "    print(f\"    - F1-Score:  {f1[i]:.4f}\")\n",
    "\n",
    "print(\"\\n💡 Next Steps:\")\n",
    "print(\"  1. Review confusion matrix to see which classes are confused\")\n",
    "print(\"  2. Check misclassified_samples.csv to analyze errors\")\n",
    "print(\"  3. Run day4_03_predictions_analysis.ipynb to visualize predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
