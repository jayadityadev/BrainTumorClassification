"""
Fast Fine-tuning: Add Kaggle dataset to existing 95.91% model
Uses existing weights as starting point - much faster than full retraining!
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from PIL import Image
from tqdm import tqdm

# Configuration
EXISTING_MODEL_PATH = 'outputs/models/center_crop_fixed/densenet121_center_crop_fixed_20251025_215730.keras'
TRAIN_CSV = 'outputs/combined_data_splits/train_split.csv'
TEST_CSV = 'outputs/combined_data_splits/test_split.csv'
OUTPUT_DIR = 'outputs/models/kaggle_finetuned'
IMG_SIZE = (128, 128)
BATCH_SIZE = 32
FINETUNE_EPOCHS = 10
LEARNING_RATE = 1e-5  # Very low to preserve existing knowledge
NUM_CLASSES = 3
CLASS_NAMES = ['Glioma', 'Meningioma', 'Pituitary']

# GPU Configuration
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def create_data_generator(csv_path, batch_size, shuffle=True, augment=False):
    """Create data generator from CSV file"""
    df = pd.read_csv(csv_path)
    
    def data_generator():
        indices = np.arange(len(df))
        while True:
            if shuffle:
                np.random.shuffle(indices)
            
            for start_idx in range(0, len(df), batch_size):
                batch_indices = indices[start_idx:start_idx + batch_size]
                batch_images = []
                batch_labels = []
                
                for idx in batch_indices:
                    row = df.iloc[idx]
                    img_path = row['filepath']
                    label = row['label']
                    
                    try:
                        # Load image (already preprocessed to RGB)
                        img = Image.open(img_path)
                        img_array = np.array(img) / 255.0
                        
                        # Data augmentation for training
                        if augment:
                            # Random horizontal flip
                            if np.random.random() > 0.5:
                                img_array = np.fliplr(img_array)
                            
                            # Random rotation (-10 to +10 degrees)
                            if np.random.random() > 0.5:
                                angle = np.random.uniform(-10, 10)
                                img_pil = Image.fromarray((img_array * 255).astype(np.uint8))
                                img_pil = img_pil.rotate(angle, fillcolor=0)
                                img_array = np.array(img_pil) / 255.0
                            
                            # Random brightness adjustment
                            if np.random.random() > 0.5:
                                brightness_factor = np.random.uniform(0.85, 1.15)
                                img_array = np.clip(img_array * brightness_factor, 0, 1)
                        
                        batch_images.append(img_array)
                        batch_labels.append(label)
                        
                    except Exception as e:
                        print(f"Error loading {img_path}: {e}")
                        continue
                
                if len(batch_images) > 0:
                    yield (
                        np.array(batch_images, dtype=np.float32),
                        tf.keras.utils.to_categorical(batch_labels, NUM_CLASSES)
                    )
    
    return data_generator

def fast_finetune():
    """Fine-tune existing model with combined dataset"""
    
    print("="*80)
    print("üöÄ FAST FINE-TUNING: Adding Kaggle Dataset to Existing Model")
    print("="*80)
    print()
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Load data info
    train_df = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)
    
    print(f"üìä Dataset Statistics:")
    print(f"   Training samples: {len(train_df)}")
    print(f"   Testing samples: {len(test_df)}")
    print()
    
    print(f"üí° Strategy:")
    print(f"   ‚úÖ Load existing 95.91% accuracy model")
    print(f"   ‚úÖ Fine-tune with combined dataset (gentle learning)")
    print(f"   ‚úÖ Preserve brain tumor knowledge")
    print()
    
    # Calculate steps
    steps_per_epoch = len(train_df) // BATCH_SIZE
    validation_steps = len(test_df) // BATCH_SIZE
    
    print(f"üî¢ Training Configuration:")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Steps per epoch: {steps_per_epoch}")
    print(f"   Validation steps: {validation_steps}")
    print(f"   Fine-tuning epochs: {FINETUNE_EPOCHS}")
    print(f"   Learning rate: {LEARNING_RATE} (very gentle!)")
    print()
    
    # Load existing model
    print(f"üì• Loading existing model...")
    print(f"   Path: {EXISTING_MODEL_PATH}")
    
    if not os.path.exists(EXISTING_MODEL_PATH):
        print(f"‚ùå Error: Model not found at {EXISTING_MODEL_PATH}")
        return
    
    model = keras.models.load_model(EXISTING_MODEL_PATH)
    print(f"‚úÖ Loaded existing model with {model.count_params():,} parameters")
    print(f"   Starting accuracy: 95.91% (on .mat dataset)")
    print()
    
    # Unfreeze all layers for fine-tuning
    for layer in model.layers:
        layer.trainable = True
    
    # Compile with very low learning rate
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print(f"üî• All layers unfrozen for gentle fine-tuning")
    print()
    
    # Create data generators
    print("üìÇ Creating data generators...")
    train_generator = create_data_generator(
        TRAIN_CSV, 
        BATCH_SIZE, 
        shuffle=True, 
        augment=True
    )
    val_generator = create_data_generator(
        TEST_CSV, 
        BATCH_SIZE, 
        shuffle=False, 
        augment=False
    )
    
    # Create TF datasets
    train_dataset = tf.data.Dataset.from_generator(
        train_generator,
        output_signature=(
            tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, NUM_CLASSES), dtype=tf.float32)
        )
    ).prefetch(tf.data.AUTOTUNE)
    
    val_dataset = tf.data.Dataset.from_generator(
        val_generator,
        output_signature=(
            tf.TensorSpec(shape=(None, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, NUM_CLASSES), dtype=tf.float32)
        )
    ).prefetch(tf.data.AUTOTUNE)
    
    # Callbacks
    callbacks = [
        ModelCheckpoint(
            os.path.join(OUTPUT_DIR, f'densenet121_kaggle_finetuned_{timestamp}.keras'),
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        EarlyStopping(
            monitor='val_accuracy',
            patience=3,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=2,
            min_lr=1e-7,
            verbose=1
        ),
        CSVLogger(os.path.join(OUTPUT_DIR, f'finetuning_history_{timestamp}.csv'))
    ]
    
    print("="*80)
    print("üèãÔ∏è STARTING FINE-TUNING")
    print("="*80)
    print(f"‚è±Ô∏è  Estimated time: 20-30 minutes")
    print()
    
    history = model.fit(
        train_dataset,
        steps_per_epoch=steps_per_epoch,
        epochs=FINETUNE_EPOCHS,
        validation_data=val_dataset,
        validation_steps=validation_steps,
        callbacks=callbacks,
        verbose=1
    )
    
    # Save final model
    final_model_path = os.path.join(OUTPUT_DIR, f'densenet121_final_finetuned_{timestamp}.keras')
    model.save(final_model_path)
    print(f"üíæ Final model saved: {final_model_path}")
    
    # Evaluate on test set
    print()
    print("="*80)
    print("üìä EVALUATING FINE-TUNED MODEL")
    print("="*80)
    
    # Make predictions
    test_df = pd.read_csv(TEST_CSV)
    y_true = []
    y_pred = []
    
    print("üîÆ Making predictions on test set...")
    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc="Predicting"):
        img_path = row['filepath']
        label = row['label']
        
        try:
            img = Image.open(img_path)
            img_array = np.array(img) / 255.0
            img_batch = np.expand_dims(img_array, axis=0)
            
            pred = model.predict(img_batch, verbose=0)[0]
            pred_class = np.argmax(pred)
            
            y_true.append(label)
            y_pred.append(pred_class)
            
        except Exception as e:
            print(f"Error predicting {img_path}: {e}")
            continue
    
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # Calculate metrics
    accuracy = np.mean(y_true == y_pred)
    
    print()
    print("="*80)
    print("üìà RESULTS")
    print("="*80)
    print(f"\nüéØ Test Accuracy: {accuracy*100:.2f}%")
    print()
    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)
    plt.title(f'Fine-tuned Model - Confusion Matrix\nAccuracy: {accuracy*100:.2f}%')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    cm_path = os.path.join(OUTPUT_DIR, f'confusion_matrix_{timestamp}.png')
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Confusion matrix saved: {cm_path}")
    
    # Plot training history
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Fine-tuning Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Fine-tuning Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    history_path = os.path.join(OUTPUT_DIR, f'finetuning_history_{timestamp}.png')
    plt.savefig(history_path, dpi=300, bbox_inches='tight')
    print(f"üíæ Training history saved: {history_path}")
    
    print()
    print("="*80)
    print("‚úÖ FINE-TUNING COMPLETE!")
    print("="*80)
    print()
    print("üìä Summary:")
    print(f"   Starting model: 95.91% accuracy (on .mat only)")
    print(f"   Fine-tuned model: {accuracy*100:.2f}% accuracy (on combined dataset)")
    print(f"   Training time: ~{len(history.history['loss']) * 2} minutes")
    print(f"   Approach: Gentle fine-tuning (preserved existing knowledge)")
    print()
    print("üíæ Model saved to:")
    print(f"   {final_model_path}")
    print()
    print("üéØ Next steps:")
    print("   1. Test on Kaggle test set to verify generalization")
    print("   2. Update inference scripts to use new model")
    print("   3. Deploy to web application")
    print("="*80)
    
    return model, history, accuracy

if __name__ == "__main__":
    model, history, accuracy = fast_finetune()
